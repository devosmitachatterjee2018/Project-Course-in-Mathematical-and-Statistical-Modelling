{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize all tracks in file '2017-05-11_00h_tracks.csv' :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ID                              Time        x        y\n",
      "133578  140016899  2017-05-11 07:41:54.445000+02:00 -6.34335  12.9815\n",
      "133579  140016899  2017-05-11 07:41:54.699000+02:00 -6.11119  11.9793\n",
      "133580  140016899  2017-05-11 07:41:54.951000+02:00 -5.87181  11.1854\n",
      "133581  140016899  2017-05-11 07:41:55.204000+02:00 -5.57613  10.2882\n",
      "133582  140016899  2017-05-11 07:41:55.457000+02:00 -5.05073  9.39386\n",
      "Selected paths saved to csv !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "num_TOs = 1500\n",
    "\n",
    "# Loading original tracks data:\n",
    "cols = ['ID','Time','x', 'y','Speed','Type','Estimated']\n",
    "data = pd.read_csv('2017-05-11_00h_tracks.csv',index_col=False, header=None,sep=';',names=cols )\n",
    "data = data.drop(data.index[0])   # drops the first row\n",
    "df = data[(data['Type'] == 2)]  # only selecting TO of type 2 (car)\n",
    "\n",
    "\n",
    "# creating new data frame for plotting trajectories: \n",
    "df = df[['ID', 'Time','x', 'y']]\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# path to to put saved file:\n",
    "path = r'C:\\Users\\Katarina Tran\\Desktop\\CAS\\Project_course_math_stat_modelling\\2017_05_11_L'\n",
    "\n",
    "# the file is currently named 'selected_path.csv':\n",
    "df.to_csv(os.path.join(path, r'xy_L_2017_05_11_filtered.csv'), index=False)\n",
    "\n",
    "print('Selected paths saved to csv !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to lat-long coord:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversion completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\"\"\" Converts data x-y coordinates into latitudinal and longitudinal positions.\n",
    " Specify below the path where the csv file is to be saved. Requires Pandas and Numpy package.\"\"\"\n",
    "\n",
    "########################################################################################################################\n",
    "# lat <=> y, long <=> x\n",
    "ref_position = np.array([59.916559, 10.734520])\n",
    "lat = 59.916687\n",
    "lon = 10.734836\n",
    "position = [lat, lon]\n",
    "\n",
    "diff_lat = position[0] - ref_position[0]\n",
    "diff_lon = position[1] - ref_position[1]\n",
    "\n",
    "diff_x = 23.41\n",
    "diff_y = -20\n",
    "\n",
    "const_lat = np.round(diff_lat/diff_y, 6)\n",
    "const_lon = np.round(diff_lon/diff_x, 6)\n",
    "\n",
    "theta = np.pi + np.pi/2\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "cols = ['ID','Time','x', 'y']\n",
    "\n",
    "# Add the csv file to be converted here:\n",
    "#df = pd.read_csv(r'C:\\Users\\Katarina Tran\\Desktop\\CAS\\Project_course_math_stat_modelling\\2017_05_11_L\\xy_R_2017_05_11_filtered.csv', index_col=False, header=None, sep=',', names= cols)\n",
    "df = pd.read_csv(r'C:\\Users\\Katarina Tran\\Desktop\\CAS\\Project_course_math_stat_modelling\\2017_05_11_L\\xy_L_2017_05_11_filtered.csv')\n",
    "\n",
    "df = df.drop(df.index[0])   # drops the first row\n",
    "\n",
    "df['lat'] = df['y'].astype(float)\n",
    "df['long'] = df['x'].astype(float)\n",
    "df['x'] = df['long'].astype(float)\n",
    "df['y'] = df['lat'].astype(float)\n",
    "\n",
    "# rotate x-y-coordinates by pi/2:\n",
    "#df['x_new'] = df['x'].multiply(np.cos(theta)) - df['y'].multiply(np.sin(theta))\n",
    "#df['y_new'] = df['x'].multiply(np.sin(theta)) + df['y'].multiply(np.cos(theta))\n",
    "\n",
    "df['const_lat'] = const_lat\n",
    "df['const_long'] = const_lon\n",
    "df['ref_lat'] = ref_position[0]\n",
    "df['ref_long'] = ref_position[1]\n",
    "\n",
    "df['lat'] = df['y'].multiply(df['const_lat'])\n",
    "df['lat'] = df['lat'].add(df['ref_lat'])\n",
    "\n",
    "df['long'] = df['x'].multiply(df['const_long'])\n",
    "df['long'] = df['long'].add(df['ref_long'])\n",
    "\n",
    "#df = df.drop(['const_lat', 'const_long', 'ref_lat', 'ref_long', 'x', 'y', 'x_new', 'y_new'], axis=1)\n",
    "df = df.drop(['const_lat', 'const_long', 'ref_lat', 'ref_long', 'x', 'y'], axis=1)\n",
    "\n",
    "# rounding positions coordinates to 6 decimals:\n",
    "df = df.round({'lat': 6, 'long': 6})\n",
    "\n",
    "# path to to put saved file:\n",
    "path = r'C:\\Users\\Katarina Tran\\Desktop\\CAS\\Project_course_math_stat_modelling\\2017_05_11_L'\n",
    "\n",
    "# the file is currently named 'LatLongData.csv':\n",
    "df.to_csv(os.path.join(path, r'LatLongData_2017_05_11_L.csv'), index=False)\n",
    "\n",
    "print('conversion completed!')\n",
    "df_converted = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating geojson file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geojson created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "''' This script converts the a data frame with columns: 'ID', 'lat', long' into geojson file for plotting '''\n",
    "\n",
    "########################################################################################################################\n",
    "'''\n",
    "# example data:    note: the 'lat' and 'long' must not be of typ numpy.int64 !\n",
    "Data = {'ID': [1, 1, 1, 1, 2, 2, 2, 2, 2, 2], 'lat':  [10.734221, 10.734474, 10.734661, 10.734940,  10.734114,\n",
    "                                                       10.734527, 10.734436, 10.734168, 10.733894, 10.733749],\n",
    "        'long': [59.916174, 59.916397, 59.916545, 59.916757,  59.916206, 59.916521, 59.916615, 59.916679, 59.916765,\n",
    "                 59.916798]\n",
    "        }\n",
    "df = DataFrame(Data, columns=['ID', 'lat', 'long'])\n",
    "\n",
    "'''\n",
    "########################################################################################################################\n",
    "cols = ['ID','Time','lat', 'long']\n",
    "# Add the csv file to be converted here:\n",
    "df = pd.read_csv(r'C:\\Users\\Katarina Tran\\Desktop\\CAS\\Project_course_math_stat_modelling\\2017_05_11_L\\LatLongData_2017_05_11_L.csv', index_col=False, header=None, sep=',', names=cols)\n",
    "df = df.drop(df.index[0])   # drops the first row\n",
    "del df['Time']\n",
    "#df = df.loc[:, ['ID', 'lat', 'long']]\n",
    "\n",
    "\n",
    "# choose number of traffic objects to plot:\n",
    "num_TO = 900\n",
    "TO_unique = df.ID.unique()\n",
    "batch = TO_unique[0:num_TO]\n",
    "df = df[df['ID'].isin(batch)]\n",
    "\n",
    "df['lat'] = df['lat'].astype(float)\n",
    "df['long'] = df['long'].astype(float)\n",
    "def df_to_geojson(df):\n",
    "    # extracting unique TO IDs:\n",
    "    TO_unique = df.ID.unique()\n",
    "    MultiLineString = []\n",
    "    for object in TO_unique:\n",
    "        # converts the the data frame to an array  [[ , ],[ , ],...[ , ]] :\n",
    "        string = df.loc[df['ID'] == object].values\n",
    "        # deleting first element in the array (corresponding to the ID value):\n",
    "        string = np.delete(string, np.s_[0], 1)\n",
    "        num_rows = len(string)\n",
    "        object_string = []\n",
    "        for row in range(num_rows):\n",
    "            string[row][0] = np.round(string[row][0], 6)\n",
    "            string[row][1] = np.round(string[row][1], 6)\n",
    "            object_string.append([string[row][1], string[row][0]])\n",
    "        MultiLineString.append(object_string)\n",
    "\n",
    "    geojson_dict = {'type': 'FeatureCollection', 'features': [{'type': 'Feature', 'properties': {}, 'geometry':\n",
    "        {'type':'MultiLineString', 'coordinates': MultiLineString}}]}\n",
    "\n",
    "    return geojson_dict\n",
    "\n",
    "geojson_dict = df_to_geojson(df)\n",
    "\n",
    "geojson_str = json.dumps(geojson_dict, indent=2)\n",
    "\n",
    "# save the geojson result to a file\n",
    "output_filename = 'TO_paths_2017_05_11_L.geojson'\n",
    "\n",
    "\n",
    "#with open(output_filename, 'wb') as output_file:\n",
    "with open(output_filename, 'w') as output_file:\n",
    "    output_file.write('{}'.format(geojson_str))\n",
    "\n",
    "print('geojson created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for separating left turn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katarina Tran\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Katarina Tran\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Katarina Tran\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Katarina Tran\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected paths saved to csv !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "theta = -0.523   #radians\n",
    "\n",
    "theta2 = 0.785   \n",
    "\n",
    "# Loading original tracks data:\n",
    "cols = ['ID','Time','x', 'y','Speed','Type','Estimated']\n",
    "#cols = ['ID','Time','x', 'y']\n",
    "data = pd.read_csv('2017-05-11_00h_tracks.csv',index_col=False, header=None,sep=';',names=cols )\n",
    "data = data.drop(data.index[0])   # drops the first row\n",
    "df1 = data[(data['Type'] == 2)]  # only selecting TO of type 2 (car)\n",
    "\n",
    "# rotate x-y-coordinates by theta to align the road with the x-y axis in the map:\n",
    "df1['x_rot'] = df1['x'].multiply(np.cos(theta)) - df1['y'].multiply(np.sin(theta))\n",
    "df1['y_rot'] = df1['x'].multiply(np.sin(theta)) + df1['y'].multiply(np.cos(theta))\n",
    "\n",
    "# rotate x-y-coordinates by theta to align the road with the x-y axis in the map:\n",
    "df1['x_rot2'] = df1['x'].multiply(np.cos(theta2)) - df1['y'].multiply(np.sin(theta2))\n",
    "df1['y_rot2'] = df1['x'].multiply(np.sin(theta2)) + df1['y'].multiply(np.cos(theta2))\n",
    "\n",
    "\n",
    "\n",
    "# creating new data frame for plotting trajectories: \n",
    "df = pd.DataFrame(columns=['ID', 'Time', 'x', 'y'])\n",
    "\n",
    "grouped = df1.groupby(['ID'])   # selecting only TO ID:s that fullfill x-y coord. condition for right turn\n",
    "for ID,group in grouped:\n",
    "\n",
    "    df_temp = pd.DataFrame({\n",
    "                            'x_cond1':[not(group['x'] > 0).any()]\n",
    "                            ,'x_cond2':[(group['x_rot'] > -3).any()]\n",
    "                            ,'x_cond3':[(group['x'] < -23).any()]  \n",
    "                            ,'x_cond4':[not(group['x_rot2'] > 3).any()]  \n",
    "                            ,'x_cond5':[not(group['x_rot2'] > 4).any()]\n",
    "                            ,'x_cond6':[(group['x_rot2'] > -3).any()]\n",
    "                            ,'y_cond1':[not(group['y_rot'] < -14).any()]\n",
    "                            ,'y_cond2':(group['y_rot'] > 2).any()\n",
    "                            ,'y_cond3':[(group['y'] < -10).any()] \n",
    "                            ,'y_cond4':[(group['y_rot'] < -3.5).any()]\n",
    "                            ,'y_cond5':[(group['y_rot'] < -3.5).any()]\n",
    "                           })\n",
    "    \n",
    "    if df_temp.all(axis=None):   # if boundary conditions for x and y are fullfilled\n",
    "        df = df.append(group[['ID','Time','x','y']])\n",
    "        \n",
    "\n",
    "#df = df.rename(columns={\"x_rot2\": \"x\", \"y_rot2\": \"y\"})\n",
    "#df = df.iloc[:,[0,1,3,5]]\n",
    "\n",
    "# path to to put saved file:\n",
    "path = r'C:\\Users\\Katarina Tran\\Desktop\\CAS\\Project_course_math_stat_modelling\\2017_05_11_L'\n",
    "\n",
    "# the file is currently named 'selected_path.csv':\n",
    "df.to_csv(os.path.join(path, r'xy_L_2017_05_11_filtered.csv'), index=False)\n",
    "\n",
    "print('Selected paths saved to csv !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for separating right turn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katarina Tran\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Katarina Tran\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-079a352d89af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mgrouped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# selecting only TO ID:s that fullfill x-y coord. condition for right turn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[0;32m   7630\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[0;32m   7631\u001b[0m                        \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7632\u001b[1;33m                        observed=observed, **kwargs)\n\u001b[0m\u001b[0;32m   7633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7634\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(obj, by, **kwds)\u001b[0m\n\u001b[0;32m   2108\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'invalid type: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2110\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5136\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5138\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   5125\u001b[0m         \"\"\"\n\u001b[0;32m   5126\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5127\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5128\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5129\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m   5134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5135\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5136\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5138\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[0mbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   1897\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1898\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[1;32m-> 1899\u001b[1;33m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[0;32m   1900\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[0;32m   3148\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3150\u001b[1;33m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "#num_TOs = 1500\n",
    "x_left_bound = 5\n",
    "x_right_bound = 33\n",
    "y_upper_bound = -30\n",
    "y_lower_bound = 2\n",
    "\n",
    "x_condition_2 = 22\n",
    "y_condition_2 = -15\n",
    "theta = 0.523   #radians\n",
    "\n",
    "\n",
    "# Loading original tracks data:\n",
    "cols = ['ID','Time','x', 'y','Speed','Type','Estimated']\n",
    "data = pd.read_csv('2017-05-11_00h_tracks.csv',index_col=False, header=None,sep=';',names=cols )\n",
    "data = data.drop(data.index[0])   # drops the first row\n",
    "df1 = data[(data['Type'] == 2)]  # only selecting TO of type 2 (car)\n",
    "\n",
    "# rotate x-y-coordinates by theta to align the road with the x-y axis in the map:\n",
    "df1['x_rot'] = df1['x'].multiply(np.cos(theta)) - df1['y'].multiply(np.sin(theta))\n",
    "df1['y_rot'] = df1['x'].multiply(np.sin(theta)) + df1['y'].multiply(np.cos(theta))\n",
    "\n",
    "\n",
    "# creating new data frame for plotting trajectories: \n",
    "df = pd.DataFrame(columns=['ID', 'Time', 'x', 'y'])\n",
    "print('test')\n",
    "grouped = df1.groupby(['ID'])   # selecting only TO ID:s that fullfill x-y coord. condition for right turn\n",
    "for ID,group in grouped:\n",
    "\n",
    "    df_temp = pd.DataFrame({#'x_cond':group['x'].between(x_left_bound, x_right_bound),\n",
    "                            #'y_cond':group['y'].between(y_upper_bound, y_lower_bound),\n",
    "                            'x_cond1':[not(group['x'] > 0).any()]\n",
    "                            ,'x_cond2':[not(group['x_rot'] > 2).any()]\n",
    "                            ,'x_cond3':[not(group['x_rot'] < -15).any()]  \n",
    "                            #,'y_cond1':[not(group['y_rot'] < -4).any()]\n",
    "                            #,'y_cond2':(group['y_rot'] > 1).any()\n",
    "                            ,'x_cond3':[(group['x_rot'] <= -22).any()] \n",
    "                           # ,'x_cond4':[not(group['x_rot'] > -7).any()]\n",
    "                            #,'x_cond5':[not(group['x'] >-10).any()]\n",
    "                            #,'y_cond3':not(group['y_rot'] > 13).any()\n",
    "                            \n",
    "                           })\n",
    "    \n",
    "    if df_temp.all(axis=None):   # if boundary conditions for x and y are fullfilled\n",
    "        df = df.append(group[['ID','Time','x','y','Type']])    \n",
    "#df = df.rename(columns={\"x_rot\": \"x\", \"y_rot\": \"y\"})\n",
    "#df = df.iloc[:,[0,1,3,5]]\n",
    "print(df.head())\n",
    "# path to to put saved file:\n",
    "path = r'C:\\Users\\Katarina Tran\\Desktop\\CAS\\Project_course_math_stat_modelling\\2017_05_11_L'\n",
    "\n",
    "# the file is currently named 'selected_path.csv':\n",
    "df.to_csv(os.path.join(path, r'xy_L_2017_05_11_filtered2.csv'), index=False)\n",
    "\n",
    "print('Selected paths saved to csv !')\n",
    "\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if time matches between state vector file and xy-file , seems like state vector is ~ 6.5 seconds ahead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID                              Time\n",
      "1  140016900  2017-05-11 07:41:56.871000+02:00\n",
      "2  140016900  2017-05-11 07:41:57.123000+02:00\n",
      "3  140016900  2017-05-11 07:41:57.376000+02:00\n",
      "4  140016900  2017-05-11 07:41:57.629000+02:00\n",
      "5  140016900  2017-05-11 07:41:57.880000+02:00\n",
      "\n",
      "\n",
      "new\n",
      "              ID  Time id              State time\n",
      "32265  140016900  92386.0 2017-05-11 07:41:56.860\n",
      "32266  140016900  92387.0 2017-05-11 07:41:57.160\n",
      "32267  140016900  92388.0 2017-05-11 07:41:57.460\n",
      "32268  140016900  92389.0 2017-05-11 07:41:57.760\n",
      "32269  140016900  92390.0 2017-05-11 07:41:58.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katarina Tran\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Katarina Tran\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Katarina Tran\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "# Loading original tracks data:\n",
    "data = pd.read_csv(r'C:\\Users\\Katarina Tran\\Desktop\\CAS\\Project_course_math_stat_modelling\\2017_05_11_L\\xy_L_2017_05_11_filtered.csv')\n",
    "\n",
    "data = data.drop(data.index[0])   # drops the first row\n",
    "\n",
    "\n",
    "df1 = data[['ID','Time']]\n",
    "\n",
    "# Loading data with occupancy grid:\n",
    "data2 = pd.read_csv(r'C:\\Users\\Katarina Tran\\Desktop\\CAS\\Project_course_math_stat_modelling\\2017_05_11_L\\result_2017_05_11_L.csv')\n",
    "\n",
    "data2['ID'] = data2['Object id']\n",
    "\n",
    "del data2['Object id']\n",
    "del data2['State time']\n",
    "\n",
    "\n",
    "# choosing ID:s in df2 that also exists in df1:\n",
    "df2 = data2.loc[data2['ID'].isin(df1['ID'])]\n",
    "df2['ID'] = df2['ID'].astype('int32')\n",
    "#print(df2.dtypes)\n",
    "# choosing ID:s in df1 that is also existing in df2:\n",
    "df1 = df1.loc[df1['ID'].isin(df2['ID'])]\n",
    "df2 = df2[['ID', 'Time id']]\n",
    "print(df1.head())\n",
    "\n",
    "\n",
    "temp = df2.loc[df2['ID']==140016900]\n",
    "\n",
    "# calculating state time for data containing occupancy grid:\n",
    "temp['Time id'] = temp['Time id'].astype(float)\n",
    "start_time = pd.to_datetime('2017-05-11 00:00:07.560') \n",
    "temp['State time'] = start_time + timedelta(seconds=0.3)*temp['Time id'] - timedelta(seconds= 6.5)\n",
    "\n",
    "print('\\n')\n",
    "print('new')\n",
    "print(temp.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting state vector (LEFT turn) which corresponds in time with xy-file cut in approximately same position :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Given date string not likely a datetime.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime_to_datetime64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1862\u001b[0m             \u001b[1;31m# If tzaware, these values represent unix timestamps, so we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslibs/conversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.datetime_to_datetime64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unrecognized value type: <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-193-6e8c7b97ff1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Time'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Time'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'datetime64[ns]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mABCDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMutableMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, box, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdayfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myearfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myearfirst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[0mutc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mutc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_iso8601\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m             allow_object=True)\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[0;32m   1864\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz_parsed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1866\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1868\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[0;32m   1855\u001b[0m             \u001b[0mdayfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1856\u001b[0m             \u001b[0myearfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myearfirst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1857\u001b[1;33m             \u001b[0mrequire_iso8601\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1858\u001b[0m         )\n\u001b[0;32m   1859\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime_object\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime_object\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslibs/parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Given date string not likely a datetime."
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "minimum_sequence = 15  # sequence of xy-trajectories for map plotting \n",
    "# num_TOs = 1500\n",
    "\n",
    "# Loading file which contains x-y coord for right turn paths:\n",
    "cols = ['ID','Time','x','y']\n",
    "data1 = pd.read_csv(r'C:\\Users\\Katarina Tran\\Desktop\\CAS\\Project_course_math_stat_modelling\\2017_05_11_L\\xy_L_2017_05_11_filtered.csv',index_col=False, header=None,sep=',',names=cols )\n",
    "data1 = data1.drop(data1.index[0])   # drops the first row\n",
    "#df1 = data1[(data1['Type'] == 2)]  # only selecting TO of type 2 (car), allready filltered out above\n",
    "df1 = data1\n",
    "\n",
    "df1['Time'] = pd.to_datetime(df1['Time']) \n",
    "df1['Time'] = df1['Time'].astype('datetime64[ns]')\n",
    "df1['ID'] = df1['ID'].astype('int32')\n",
    "df1['ID'] = pd.to_numeric(df1['ID'])\n",
    "df1['x_numeric'] = pd.to_numeric(df1['x'])\n",
    "\n",
    "# deleting xy-rows which has x-coord. above 18:\n",
    "indexRows = df1[ df1['x_numeric'] >= 18].index\n",
    "df1.drop(indexRows , inplace=True)\n",
    "del df1['x_numeric']\n",
    "# Loading data with occupancy grid:\n",
    "data2 = pd.read_csv('result_2017_05_11_L.csv')\n",
    "data2['ID'] = data2['Object id']\n",
    "del data2['Object id']\n",
    "del data2['State time']\n",
    "\n",
    "\n",
    "# choosing ID:s in df2 that also exists in df1:\n",
    "df2 = data2.loc[data2['ID'].isin(df1['ID'])]\n",
    "df2['ID'] = df2['ID'].astype(float)\n",
    "#print(df2.dtypes)\n",
    "# choosing ID:s in df1 that is also existing in df2:\n",
    "df1 = df1.loc[df1['ID'].isin(df2['ID'])]\n",
    "\n",
    "\n",
    "\n",
    "# calculating state time for data containing occupancy grid:\n",
    "df2['Time id'] = df2['Time id'].astype(float)\n",
    "start_time = pd.to_datetime('2017-05-11 00:00:07.560') \n",
    "df2['State time'] = start_time + timedelta(seconds=0.3)*df2['Time id'] - timedelta(seconds=6.5)\n",
    "\n",
    "df_state = df2\n",
    "\n",
    "# path to to put saved file:\n",
    "path = r'C:\\Users\\Katarina Tran\\Desktop\\CAS\\Project_course_math_stat_modelling\\2017_05_11_L'\n",
    "\n",
    "# the file is currently named 'xy_right_time_matching.csv':\n",
    "df.to_csv(os.path.join(path, r'xy_L_2017_05_11_filtered.csv'), index=False)\n",
    "\n",
    "# the file is currently named 'state_vector_right_turn.csv':\n",
    "df_state.to_csv(os.path.join(path, r'state_vect_L_2017_05_11.csv'), index=False)\n",
    "\n",
    "print('Selected paths saved to csv !')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
